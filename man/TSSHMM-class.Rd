% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TSSHMM-class.R
\docType{class}
\name{TSSHMM-class}
\alias{TSSHMM-class}
\alias{TSSHMM}
\alias{parameters}
\alias{parameters,TSSHMM-method}
\alias{parameters<-}
\alias{parameters<-,TSSHMM-method}
\alias{show,TSSHMM-method}
\alias{train}
\alias{train,TSSHMM,GRanges,GRanges-method}
\alias{viterbi}
\alias{viterbi,TSSHMM,GRanges,GRanges-method}
\title{Transcription start site (TSS) hidden Markov model (HMM).}
\usage{
parameters(model)

\S4method{parameters}{TSSHMM}(model)

parameters(model) <- value

\S4method{parameters}{TSSHMM}(model) <- value

\S4method{show}{TSSHMM}(object)

train(model, signal, bg)

\S4method{train}{TSSHMM,GRanges,GRanges}(model, signal, bg)

viterbi(model, signal, bg, ...)

\S4method{viterbi}{TSSHMM,GRanges,GRanges}(model, signal, bg, tol = 0.001, n = 200)
}
\arguments{
\item{model}{S4 object of a pre-designed hidden Markov model.}

\item{value}{named list containing 2 model matrices: the transition state
probability matrix named "trans", and the discrete observable emission
probability matrix named "emis".}

\item{object}{S4 object to display.}

\item{signal}{Stranded, single base \code{GRanges} with integer score.}

\item{bg}{Stranded, single base \code{GRanges} with integer score.}

\item{...}{Optional arguments passed on from generics to methods.}

\item{tol}{Relative change to HMM steady state used to calculate
intra-promoter distance.  Preference should be given to
adjusting the \code{n} windows parameter before adjusting this.}

\item{n}{Maximum number of background windows after a promoter,
beyond which one is certain that the promoter has ended.  See
the appendix in the vignette to understand the theory behind
using \code{n} and \code{tol} in this calculation.  \code{n} is the upper
limit; the actual number of background windows is calculated
from the model.}
}
\description{
Initialize, train, and decode nascent RNA reads to find transcription start
sites using Andre Martin's 2014 hidden Markov model for PRO-cap reads and
Pariksheet Nanda's extension for PRO-seq background reads.
}
\details{
The model takes inputs of raw nascent RNA counts, aggregates 10 basepair
tiles by maximum value, and categorizes them into 3 types of observations:
\enumerate{
\item no signal (TAP+ == 0)
\item enriched  (TAP+ >  TAP-)
\item depleted  (TAP- >= TAP+ > 0)
}
...and searches for 3 groups of hidden states:
\enumerate{
\item background,
\item peaked TSS regions, and
\item non-peaked TSS regions
}

Both the peaked and non-peaked TSS regions each require 3 states to describe
them:
\itemize{
\item Non-peaked regions flanked by single low intensity transitions and a
moderately long intensity center.
\item Peaked regions flanked by one-or-more low intensity transitions and a
short intense center.
}
Therefore, in total the HMM has 7 states for PRO-cap and an additional state
for PRO-seq:
\describe{
\item{B}{Background}
\item{N1}{Non-peaked TSS transition state}
\item{N2}{Non-peaked TSS repeating state}
\item{N3}{Non-peaked TSS transition state}
\item{P1}{Peaked TSS moderate signal}
\item{P2}{Peaked TSS high signal}
\item{P3}{Peaked TSS moderate signal}
\item{GB}{Gene-body signal in PRO-seq only}
}

Finally, after the hidden states are obtained from Viterbi decoding, the
hidden states are converted back into stranded GRanges.
}
\section{Constructor}{


model <- new("TSSHMM")
model <- new("TSSHMM", bg_proseq = TRUE)

\code{new("TSSHMM")} returns a default model object to be trained and then used
for decoding.  An optional argument \code{bg_proseq} uses PRO-seq for background
instead of PRO-cap; the default is \code{bg_proseq = FALSE}.

The TSS HMM model is implemented using the General Hidden Markov Model
(GHMM) C library.  Due to the complexity of the GHMM's C-interface, the R
wrapper provides no setters to modify the number of states or the number
of observations of the model.
}

\section{Accessors}{


\code{parameters(model)}, \code{parameters(model) <- list(trans = ..., emis = ...)}
gets or sets 2 matrices: the transition state probability matrix, and the
discrete observable emission probability matrix.

After training a model, you may wish to save the parameters to later reload
to eliminiate retraining the model in the future.
}

\section{Displaying}{


\code{show(model)} summarizes \code{parameters(model)} and the parameter dimesions,
namely, the number of transition states and number of discrete observable
emissions.
}

\section{Model Training}{


\code{train(model, signal, background)} returns the model with trained transition
and emission probabilities.  The arguments, \code{signal} and \code{bg}, are stranded,
single base \code{GRanges} with integer scores.

After training, you may wish to save the parameters so that they can be
reloaded in a later session as explained in the examples.

To train the model, the sparse reads of the signal and background need to be
exploded into dense encoded windows categorized as either enriched, depleted
or no-read observations, which are then processed by the Baum-Welch EM
algorithm to update the model transition and emission probabilities.

The training data are randomized and divided into batches large enough to
provide sufficient samples for the calculating the background state
transitions, but small enough to work within numerical precision limits and
to make the training process more observable.  On each batch, the input
training data is transformed into dense training observations and then the
Baum-Welch algorithm is run.  After each batch, the model state is shown
alongside a time estimate to complete training if the logging level has not
been reduced from the INFO level.
}

\section{Model Evaluation}{


\code{viterbi(model, signal, background, tol = 1e-3, n = 200)} returns
\code{GRanges} of active promoter or enhancer regions along with the
decoded hidden states for each window.  The \code{tol} and \code{n}
parameters are passed on to the intra-promoter distance estimation
function that add flanking windows to ensure that no cluster of proximal
promoters is broken up.

Running \code{flog.threshold(DEBUG)} before running \code{viterbi} logs additional
information of what the function is doing.
}

\section{Coercion}{


\code{as(model, "character")} or \code{as.character(model)} compactly pastes
transition and emission matrices in the same line for logging.
}

\examples{
# Model evaluation.
signal <- GRanges(paste0("chr1:", c(100, 110, 200, 300), ":+"))
score(signal) <- rep(5L, 4)
signal
(bg <- GRanges())
(model <- new("TSSHMM"))
(promoters_peaked <- viterbi(model, signal, bg))
# There is only 1 promoter in thise region because the first two signal
# values filling 2x 20 bp windows are captured by the HMM as a promoter.
# The remaining 2 signal peaks with no surrounding signal are ignored.
stopifnot(length(promoters) == 1)

# Model training with saving and reloading parameters.
#train(model, signal, bg)
(params <- parameters(model))
\dontrun{
save(params, file = "model_trained_on_foo_dataset.RData")
# ... in a later R session.
load(file = "model_trained_on_foo_dataset.RData")
}
(model <- new("TSSHMM"))
parameters(model) <- params
model
}
\references{
\insertRef{core_analysis_2014}{tsshmm}
}
\seealso{
\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4254663/}
}
