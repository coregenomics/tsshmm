---
title: TSS HMM User Guide
author:
- name: Pariksheet Nanda
  affiliation: University of Connecticut
  email: pariksheet.nanda@uconn.edu
abstract: |
  De novo discovery of gene promoter regions and transcription start sites
  (TSSs) from nuclear run-on and sequencing data.  Implements the hidden Markov
  model and TSS peak finder from Core 2014, and optionally extends the model to
  use standard PRO-seq data for the background reference instead of needing to
  collect a bespoke PRO-cap background reference.
package: tsshmm
output:
  BiocStyle::pdf_document: default
header-includes:
- \usepackage{tikz}
- \usetikzlibrary{shapes.arrows,positioning}
vignette: >
  %\VignetteIndexEntry{tsshmm-userguide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
params:
  cache: TRUE
---

```{r Setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

## Model

The sparse reads from nascent RNA sequencing assays need to be smoothed to
ascribe regions of active transcription.  Traditionally, this smoothing is done
using hidden Markov models.  This package implements the published hidden
Markov model\cite{core_analysis_2014} illustrated in the top half of Figure
\ref{fig:hmm}, as well as a novel variant of the model with an additional gene
body (G) state to negate collecting the assay-specific background reference.

\begin{smallfigure}
  \begin{tikzpicture}[
  state/.style={draw,circle,minimum size=1cm},
  dup/.style={black!20,draw,circle,minimum size=1cm}
  ]
    % States
    \node [state] (b) {B};
    \node [state,label=Non-peaked TSS,left=of b] (n1) {N1};
    \node [state,label=Peaked TSS,right=of b]    (p1) {P1};
    \node [state,below=of n1] (n2) {N2};
    \node [state,below=of p1] (p2) {P2};
    \node [state,below=of n2] (n3) {N3};
    \node [state,below=of p2] (p3) {P3};

	% States - additional
    \node [dup,below=of n3]     (n1-dup) {N1};
    \node [dup,below=of n1-dup] (n2-dup) {N2};
    \node [dup,below=of n2-dup] (n3-dup) {N3};
    \node [dup,right=of n1-dup] (b-dup)  {B};
    \node [dup,right=of b-dup]  (p1-dup) {P1};
    \node [dup,below=of p1-dup] (p2-dup) {P2};
    \node [dup,below=of p2-dup] (p3-dup) {P3};

	% State - new
    \node [state,right=of n3-dup] (g) {G};

    % Connections
    \draw[->]  (b) edge [loop above] node {0.99} (b);
    \draw[->]  (b) -- node [below] {0.005} (n1);
    \draw[->]  (b) -- node [below] {0.005} (p1);

    \draw[->] (n1) -- node [left] {1}   (n2);
    \draw[->] (n2) -- node [left] {0.5} (n3);
    \draw[->] (n2) edge [loop left] node {0.5} (n2);

    \draw[->] (p1) -- node [right] {0.5} (p2);
    \draw[->] (p1) edge [loop right] node {0.5} (p1);
    \draw[->] (p2) -- node [right] {0.45} (p3);
    \draw[->] (p2) edge [bend left] node [left] {0.45} (p1);
    \draw[->] (p2) edge [loop right] node {0.1} (p2);
    \draw[->] (p3) edge [loop right] node {0.5} (p3);

    \draw[->] (n3) -- node [near start,right] {1}   (b);
    \draw[->] (p3) -- node [near start,left]  {0.5} (b);

	% Connections - additional
    \draw[black!20,->]  (b-dup) edge [loop above] node {0.99} (b-dup);
    \draw[black!20,->]  (b-dup) -- node [below] {0.005} (n1-dup);
    \draw[black!20,->]  (b-dup) -- node [below] {0.005} (p1-dup);

    \draw[black!20,->] (n1-dup) -- node [left] {1}   (n2-dup);
    \draw[black!20,->] (n2-dup) -- node [left] {0.5} (n3-dup);
    \draw[black!20,->] (n2-dup) edge [loop left] node {0.5} (n2-dup);

    \draw[black!20,->] (p1-dup) -- node [right] {0.5} (p2-dup);
    \draw[black!20,->] (p1-dup) edge [loop right] node {0.5} (p1-dup);
    \draw[black!20,->] (p2-dup) -- node [right] {0.45} (p3-dup);
    \draw[black!20,->] (p2-dup) edge [bend left] node [left] {0.45} (p1-dup);
    \draw[black!20,->] (p2-dup) edge [loop right] node {0.1} (p2-dup);
    \draw[black!20,->] (p3-dup) edge [loop right] node {0.5} (p3-dup);

    \draw[black!20,->] (n3-dup) -- node [near start,text=black,right] {0.25}  (b-dup);
    \draw[black!20,->] (p3-dup) -- node [near start,text=black,left]  {0.125} (b-dup);

	% Connections - new
    \draw[->] (n3-dup) -- node [below] {0.75}  (g);
    \draw[->] (p3-dup) -- node [below] {0.375} (g);
    \draw[->] (g) -- node [right] {0.01} (b-dup);
    \draw[->] (g) edge [loop below] node {0.99} (g);

  \end{tikzpicture}
  \caption{\label{fig:hmm}TSS HMM for detecting enhancer transcripts.  Adapted
    from~\cite[supplementary figure 2]{core_analysis_2014}.  HMM to find
    transcription start sites from GRO-cap treated with and without the
    cap-removing enzyme, tobacco acid pyrophosphatase (TAP).  The model takes
    as input 10 basepair summed GRO-cap counts, classified into 3 observations:
    (i)~no signal, TAP+ == 0, (ii)~enriched, TAP+ $>$ TAP-, and (iii)~depleted,
    TAP- $>$ TAP+ $>$ 0.  The model searches for 3 states: (i)~background (B),
    (ii)~peaked TSS regions (P1-P3), and (iii)~non-peaked TSS regions (N1-N3).
    Both the peaked and non-peaked TSS regions each require 3 states to
    describe them because the non-peaked regions are flanked by single low
    intensity transitions and have a moderately long intensity center, whereas
    the peaked regions are flanked by one-or-more low intensity transitions and
    a short intense center.}
\end{smallfigure}

The published model name "TSS" HMM is a misnomer.  Strictly speaking, it would
have been more accurate to call the model "Promoter" HMM, because the model
itself does not find single-base transcription start sites (TSSs) for you; the
model only segments promoter regions which can then be searched for TSS peaks.
Nevertheless, we use the TSS HMM name to remain consistent with the
publication\cite{core_analysis_2014}.

## Older and newer assay names

Although the historical data used in the vignette is from the older GRO-cap and
GRO-seq assays, the same data processing in this package applies to their newer
variants, the PRO-cap and PRO-seq assays, respectively.  To remain consistent
with the historical example data, we continue to name the older assays, and so
if you are have data from the newer sequencing assays, you can substitute the
newer PRO-* assay names in your mind when reading the variable names.

## Prior art

Another notable promoter *de novo* discovery package is
PINTS\cite{yao_systematic_2021}; however PINTS is a python package and uses
different methods.  No other Bioconductor or CRAN package interprets promoter
regions from nascent RNA sequencing data.

# Organizing the input data

## Reading in converage histogram files

The model requires signal and background reads to call promoter regions, and
only signal reads to call TSS locations.  The reads must be the coverage or
histogram of mapped nascent RNA sequencing data of the biologically significant
end of the read: for PRO-cap the biologically significant end is the 5' end,
and in PRO-seq it is the 3' end.  Therefore, no extra data processing is
required; if the data has been visualized in a genome browser as a bigWig or
bedGraph file, simply import the data using the `rtracklayer` package,
concatenate the plus and minus strand reads into a single `GRanges` object, and
take the absolute value of the `score` metadata column so that all coverage
bases are assigned positive integers for their read count.

An example script to data tracks is shown below which assumes you have the
typical read coverage files used for visualization in a genome browser:

```{r Import data, eval = FALSE}
## *** Ignore this code block if you're only following along with the tutorial.

library(rtracklayer) # import

import_tracks <- function(file_plus, file_minus) {
    signal_plus <- import(file_plus)
    signal_minus <- import(file_minus)
    ## Browser negative strand tracks often have negative scores.
    score(signal_minus) <- abs(score(signal_minus))
    signal <- sort(c(signal_plus, signal_minus))
}

## Read in signal and background from pretend datasets.  Any read coverage file
## like bedgraphs or bigwigs work as is.
signal <- import_tracks("procap_plus.bedgraph.gz",
                        "procap_minus.bedgraph.gz")
bg <- import_tracks("proseq_plus.bw",
                    "proseq_minus.bw")
```

## Using chr22- subset data from Core 2014

We will use K562 cell nascent RNA sequencing data subset to hg19 chr22- to
demonstrate model training and evaluation.  The full R script used to generate
this subset `GRangesList` dataset is included with this package along with the
source code in `./data-raw/core2014.R`. Instead of using the wordy assay and
treatment names from the publication packaged `core2014` data, let's alias 3
convenience `GRanges` objects named by our model's intended use:

```{r Load tsshmm library}
library(tsshmm)
library(futile.logger) # flog.threshold, DEBUG, INFO, ERROR

## Enable verbose progress from encode_obs() and train() because they can
## take a long time to run on on big data sets.
flog.threshold(DEBUG)
```

```{r Inspect core2014 data, cache = params$cache, message = FALSE}
data(core2014)
signal <- core2014[["GROcap_wTAP"]]
bg_grocap <- core2014[["GROcap_noTAP"]]
bg_groseq <- core2014[["GROseq"]]
core2014
```

Peaking at the assay data above, we see more read locations from GRO-seq assay,
because it captures polymerases molecules at locations beyond the promoter.
We're looking at coverage data, so don't mistake the lengths as representing
the reads; they represent the bases at which one or more reads are present.  To
count the number of reads, you need to sum the scores:

```{r Read counts, dependson = "Inspect core2014 data", cache = params$cache}
## Number of GRO-cap reads.
sum(score(signal))
## Number of GRO-seq reads
sum(score(bg_groseq))
```

So there are actually more GRO-cap reads on the negative strand of chromosome
22 even though those reads are present at fewer locations than the GRO-seq
data.

Running against the full dataset using the steps above is explained below:

## Using the full data set from Core 2014

To run the model on the full dataset instead of the subset chr22-, generate the
`core2014full` `GRangesList` object with the bundled script:

```{r Full data run, eval = FALSE}
## *** Ignore this code block if you're only following along with the tutorial.

## Path to the script used to generate the example data.
file <- system.file("script", "core2014.R", package = "tsshmm",
                    mustWork = TRUE)

## Run the script. You'll probably need to install missing packages.
options(echo = TRUE)
source(file)
options(echo = FALSE)

## Load the generated full data by the core2014.R script.
load("core2014full.rda")
signal <- core2014full[["GROcap_wTAP"]]
bg_grocap <- core2014full[["GROcap_noTAP"]]
bg_groseq <- core2014full[["GROseq"]]
```

# Model training

## GRO-cap background

Training the model consists of 3 steps:

1. Create the model object with the initial parameters shown in Figure
   \ref{fig:hmm} using `TSSHMM()`,
2. Encode the read coverage counts into the 3 categories of discrete emissions
   observed --- no signal, enriched, and depleted --- using `encode_obs()`,
   and then
3. Feeding the shuffled obs into the model for training using `train()`, and
   collect the collect the `DataFrame` of the parameter changes.

```{r Train GRO-cap, dependson = "Inspect core2014 data", cache = params$cache}
## Initialize the model.
(model_grocap <- TSSHMM())

## Encode the signal and background into obs that the model can understand.
system.time(
    obs_grocap <- encode_obs(signal, bg_grocap)
)

## Train the model.
system.time(
    ## Seed for shuffling observations.
    set.seed(123)
    model_grocap <- train(model_grocap, obs_grocap)
)

## See the trained model parameters.
model_grocap

## Save the parameters so the trained model can recreated later.
params_grocap <- parameters(model_grocap)
```

## GRO-seq background

Duplicate the steps from the above GRO-cap section with 2 changes:

1. Pass the argument `bg_genebody = TRUE` when initializing the model to add the
   extra gene body (G) state and emission to the parameter matrices, and
2. Use the GRO-seq data stored in `bg_groseq` instead of `bg_grocap` to
   generate the randomized training observations.

```{r Train GRO-seq, dependson = "Inspect core2014 data", cache = params$cache}
(model_groseq <- TSSHMM(bg_genebody = TRUE))

system.time(
    obs_groseq <- encode_obs(signal, bg_groseq)
)

system.time(
    set.seed(456)
    model_groseq <- train(model_groseq, obs_groseq)
)

model_groseq

params_groseq <- parameters(model_groseq)
```

# Model parameters reuse

Training on the full dataset of\cite{core_analysis_2014} takes about an hour.
Therefore it makes sense to save the trained parameters to disk to reuse:

```{r Save and load model parameters, eval = FALSE}
## *** Ignore this code block if you're only following along with the tutorial.

## Save parameters for future use:
save(params_grocap, params_groseq, file = "tsshmm-params.RData")

## In a future R session:
load(file = "tsshmm-params.RData")
```

```{r Reinitialize models, dependson = paste0("Train GRO-", c("cap", "seq")), cache = params$cache}
## *** Ignore this code block if you're only following along with the tutorial.

## Reinitialize PRO-cap background model:
(model_grocap <- do.call(TSSHMM, params_grocap))
(model_groseq <- do.call(TSSHMM, params_groseq))
```

# Model inference

Now that the models have been fully trained on the data, we can use the models
to find promoter regions and each TSS within the promoter.

Of course, it is possible to use to detect clusters of TSSs within a single
promoter region.  However the `tss()` function does not check for this.  More
careful approaches like inter-quartile range methods are useful may be useful
for detecting alternative TSSs.

Additionally, below we directly search for TSSs within the putative promoter
regions.  But in actual practise, before running the `tss()` function, you may
wish to tweak the `prom_*` regions.  For example, if you're only interested in
enhancers, you may wish to remove annotated genes from the dataset, or overlap
the putative tracks with epigenetic tracks.

```{r Detect promoters then search for TSSs within promoters, dependson = paste0("Train GRO-", c("cap", "seq")), cache = params$cache}
(prom_grocap <- viterbi(model_grocap, signal, bg_grocap, tol = 1e-2))
(tss_grocap <- tss(signal, prom_grocap))

(prom_groseq <- viterbi(model_groseq, signal, bg_groseq, tol = 1e-2))
(tss_groseq <- tss(signal, prom_groseq))
```

An often useful optional parameter to `tss()` is returning a `Pairs` object
instead of `GRanges`.  The advantage of the `Pairs` object is that one can
filter on characteristics of the promoter region and then look at the
corresponding TSS locations and completely avoid the complexity of duplicating
operations on both the `prom_*` and `tss_*` objects:

```{r TSS pairs, dependson = "Detect promoters then search for TSSs within promoters", cache = params$cache}
## Silence progress messages.
flog.threshold(ERROR)

(pairs <- tss(signal, prom_grocap, pairs = TRUE))

## Access the TSS and promoters using first() and second() respectively.  Note
## that dplyr::first conflicts with S4Vectors::first, therefore we need to
## explicitly use the latter.
S4Vectors::first(pairs)
second(pairs)

## Subset for long promoter regions and get their TSSs.
(quantile <- quantile(width(second(pairs)), seq(0, 1, 0.05)))
pairs[width(second(pairs)) > quantile["80%"]]
## Also remove weak single count TSS peaks.
(pairs_long <- pairs[width(second(pairs)) > quantile["80%"] &
                     score(S4Vectors::first(pairs)) >= 2])
S4Vectors::first(pairs_long)
```

The reason that this useful `Pairs` object is not returned by default, is that
it might be confusing to the typical Bioconductor user who is familiar with
working with `GRanges` and not familiar with `Pairs`.  Perhaps you'll enjoy
working with `Pairs` objects of promoters and their TSSs from now on having
seen the utility of `Pairs` here!

# Comparison of TSS calls

Now that we've found the promoter regions and TSSs within each of those
promoters using both types of background data, how differet are the TSS class
within those background datasets?

```{r Compare TSS locations from using different backgrounds, dependson = "Detect promoters then search for TSSs within promoters", cache = params$cache}
pairs_grocap_all <- tss(signal, prom_grocap, pairs = TRUE)
pairs_groseq_all <- tss(signal, prom_groseq, pairs = TRUE)

filter_tss <- function(pairs)
    S4Vectors::first(pairs[
                   ## TSS covers at least 3 states.
                   width(second(pairs)) >= 30 &
                   ## Signal of at least 2 counts.
                   score(S4Vectors::first(pairs)) >= 2
               ])

tss_grocap_cmp <- filter_tss(pairs_grocap_all)
tss_groseq_cmp <- filter_tss(pairs_groseq_all)

(ol <- findOverlaps(tss_grocap_cmp, tss_groseq_cmp))
length(ol)
length(tss_grocap_cmp)
length(tss_groseq_cmp)

library(UpSetR)

upset(
    fromExpression(
        c(grocap = length(tss_grocap_cmp) - length(ol),
          groseq  = length(tss_groseq_cmp) - length(ol),
          `grocap&groseq` = length(ol))),
    order.by = "freq")
```

Therefore, using the more plentiful and convenient GRO-seq background reference
was still able to find `r sprintf("%.0f%%", 100 * length(ol) /
length(tss_grocap_cmp))` of the GRO-cap reference TSSs.

\bibliography{refs}

\appendix

# Minimum distance between promoter regions

Here we calculate how much the model influences
joining clusters of
signal (enriched or depleted)
into promoters (N1--N3, P1--P3).
There are long stretches with
background emission states in 5' capped RNA data.
Some of the background states are informative if they are close to promoters,
because it is possible for closely located enriched or depleted regions
with only a small amount of background between them
to actually be part of the same promoter or enhancer cluster.
To determine whether background states are uninformative and can be ignored,
we must first calculate the maximum number of background states upto which
the model will join nearby enriched or depleted regions into a cluster.

Knowing this minimum distance between promoter regions
allows a significant computational speed up
by removing long stretches of uninformative background states
where there is no 5' capped RNA signal.
<!-- Explain the rounding errors? -->
Furthermore, reducing the input data in this way
reduces the possibility of computational rounding errors
from otherwise running the model on long chromosomes,
or using the traditional workaround of rescaling the Viterbi calculations.
<!--
Symbols as used by Mark Stamp http://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf and
Richard A. O'Keefe http://www.cs.otago.ac.nz/cosc348/hmm/hmm.pdf
-->
\begin{align}
  \intertext{Let these symbols define our Markov chain tuple of
    $(S, \pi, A)$ where:}
  N &= \text{number of states} \\
  S &= \{s_1, \ldots, s_N\} = \text{set of states} \\
  \pi &= \{\pi_1, \ldots, \pi_N\} =
  \text{initial state distribution, $\sum_{i=1}^N\pi_i = 1$} \\
  A &= (a_{ij})_{i \in S, j \in S} = \text{state transition probability matrix,
    $\sum_{i \in S}a_{ij} = 1$} \\
  \intertext{Let these additional symbols define our HMM tuple of
    $(S,V,\pi,A,B)$}
  M &= \text{number of observation states}\\
  V &= \{v_1, \ldots, v_M\} =
  \text{set of possible observations, also called vocabulary} \\
  B &= {(b_{ij})}_{i \in V, j \in S} =
  \text{observation probabilities matrix, also called emission} \\
  T &= \text{Number of discrete time points} \\
  \mathcal{O} &= \{\mathcal{O}_1, \ldots, \mathcal{O}_T\} = \text{observation
    sequence}
  \intertext{The difference equations when we step the Markov chain are:}
  \pi^{(0)} &= \pi \\
  \pi^{(n+1)} &= A \cdot \pi^{(n)}
  \intertext{The general solution to the above equations is:}
  \pi^{(n+1)} &= A^n \cdot \pi
  \intertext{The corresponding symbol probability solution is:}
  \psi^{(n+1)} &= B \cdot A^n \cdot \pi
\end{align}
The tables for our model transition and emission probabilities
--- with zero probability values intentionally left blank for readability ---
are:
\begin{tabular}{r c c c c c c c}
  \toprule
  State & {B}   & {N1}  & {N2}  & {N3}  & {P1}  & {P2}  & {P3}  \\
  \midrule
  B     & 0.990 & 0.005 &       &       & 0.005 &       &       \\
  N1    &       &       & 1.000 &       &       &       &       \\
  N2    &       &       & 0.500 & 0.500 &       &       &       \\
  N3    & 1.000 &       &       &       &       &       &       \\
  P1    &       &       &       &       & 0.500 & 0.500 &       \\
  P2    &       &       &       &       & 0.450 & 0.100 & 0.450 \\
  P3    & 0.500 &       &       &       &       &       & 0.500 \\
  \bottomrule
\end{tabular}

\begin{tabular}{r c c c}
  \toprule
  State & {Background} & {Enriched} & {Depleted} \\
  \midrule
  Background     (B)      & 0.90 & 0.05 & 0.05 \\
  Non-peaked TSS (N1--N3) & 0.09 & 0.90 & 0.01 \\
  Peaked TSS (P1--P3) & 0.10 & 0.45 & 0.45 \\
  \bottomrule
\end{tabular}

\begin{align}
  \intertext{From the tables above, the matrices for our model are:}
  A &=
  \begin{bmatrix}
    0.990 & 0.005 &       &       & 0.005 &       &       \\
          &       & 1.000 &       &       &       &       \\
          &       & 0.500 & 0.500 &       &       &       \\
    1.000 &       &       &       &       &       &       \\
          &       &       &       & 0.500 & 0.500 &       \\
          &       &       &       & 0.450 & 0.100 & 0.450 \\
    0.500 &       &       &       &       &       & 0.500 \\
  \end{bmatrix} \\
  B^\top &=
  \begin{bmatrix}
    0.90 & 0.05 & 0.05 \\
    0.09 & 0.90 & 0.01 \\
    0.09 & 0.90 & 0.01 \\
    0.09 & 0.90 & 0.01 \\
    0.10 & 0.45 & 0.45 \\
    0.10 & 0.45 & 0.45 \\
    0.10 & 0.45 & 0.45
  \end{bmatrix} \\
  \intertext{Before we can substituting these values in the general solution, we
    need to do a little more work to calculate the power $n$ of the dense matrix
    $A^n$.  Calculating the power of large dense matrices is inefficient, but we
    can trivially calculate the power a diagonal matrix.  Therefore, we need to
    create the equivalent diagonal matrix $\Lambda$ with the eigenvalues of
    $A^n$ by factoring out eigenvector matrix $H$ and its inverse eigenvector
    matrix $H^{-1}$:}
  A &= H \cdot \Lambda \cdot H^{-1} \\
  \implies A^n &= {\left( H \cdot \Lambda \cdot H^{-1} \right)}^n \\
  &= H \cdot {(\Lambda)}^n \cdot H^{-1} \quad \because H \cdot H^{-1} = I
\end{align}

Solving in R:

```{r Fewer digits, echo = FALSE}
digits <- getOption("digits")
options(digits = 3)
```
```{r Factorize}
A <- matrix(c(.990, .005,  0,  0, .005,  0, 0,
                 0,    0,  1,  0,    0,  0, 0,
                 0,    0, .5, .5,    0,  0, 0,
                 1,    0,  0,  0,    0,  0, 0,
                 0,    0,  0,  0,   .5, .5, 0,
                 0,    0,  0,  0,  .45, .1, .45,
                .5,    0,  0,  0,    0,  0, .5), ncol = 7, byrow = T)
l <- eigen(A)$values
L <- l * diag(length(l))
H <- eigen(A)$vectors
H_inv <- solve(H)
stopifnot(all(zapsmall(H %*% H_inv) == diag(length(l))))  # Sanity check.
stopifnot(all(zapsmall(H %*% L %*% H_inv) == A))          # Sanity check.
L
H
H_inv
```
```{r Restore digits, echo = FALSE}
options(digits = digits)
```
\begin{align}
  \intertext{Now that we have all the pieces of $H, \Lambda,$ and $H^{-1}$,
             we can calculate $A^n$, and therefore we can finally solve:}
  \psi^{(n+1)} &=
  B \cdot A^n \cdot \pi \\
  &= B \cdot \left[ H \cdot \Lambda^n \cdot H^{-1} \right] \cdot \pi
\end{align}
To find the minimum distance between independent promoter regions,
we need to maximize the value of $n$ by choosing each possible initial state
with $\pi$ to provide an output of mostly background emission states.

Continuing the R code:

```{r Calculate distance between promoter regions, message = FALSE}
B <- t(matrix(c(c(.9, .05, .05),
                rep(c(.09, .9, .01), 3),
                rep(c(.1, .45, .45), 3)), ncol = 3, byrow = T))
psi <- function(pi, n = 1) c(B %*% t(H %*% L^n %*% H_inv) %*% unlist(pi))
## Try each state as the starting value.
pis <- unlist(apply(diag(7), 1, list), recursive = FALSE)
library(dplyr) # across, bind_cols, group_by, mutate, pull, summarize, %>%
library(tidyr) # expand_grid
library(purrr) # pmap
grid <- expand_grid(n = seq(1, 100), pi = pis)
dd <- pmap(grid, psi)
probs <-
  grid %>%
  bind_cols(do.call("rbind", dd) %>%
            `colnames<-`(c("B", "N", "P")) %>%
            as_tibble())
stopifnot(all(zapsmall(rowSums(probs[, -2:-1])) == 1))
diffs_df <-
  probs %>%
  group_by(pi) %>%
  mutate(across(matches("[BNP]", ignore.case = FALSE),
                ## Relative change.
                function(x) (x - lag(x)) / x))
diffs <-
  diffs_df %>%
  group_by(n) %>%
  summarize(max = max(B, N, P)) %>%
  pull(max)
## Check for relative convergence.
tols <- 10^(-3:-8)
sapply(tols, function(tol) min(which(diffs < tol)))
```
Therefore the model probabilities implicitly require at least
28 windows or 280 basepairs
between promoter regions before returning to the stationary distribution
using a relaxed tolerance of 1E-3, and
82 windows or 820 basepairs using a stricter tolerance of 1E-8.

# Session information

```{r sessionInfo, echo = FALSE}
knitr::raw_latex(toLatex(sessionInfo()))
```
